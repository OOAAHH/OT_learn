# 第三部分：材料和方法 - 核心技术实现

## 学习目标
通过本部分学习，学生将能够：
1. 掌握单细胞数据的预处理技术
2. 理解并实现ICNN网络架构
3. 设计多模态损失函数
4. 实现高效的训练策略

## 3.1 数据预处理技术

### 3.1.1 单细胞数据特点与挑战

#### 数据特征
```python
import anndata
import numpy as np
import pandas as pd
from scipy import sparse

def analyze_scrnaseq_data(adata):
    """分析单细胞RNA-seq数据的基本特征"""
    print(f"细胞数量: {adata.n_obs}")
    print(f"基因数量: {adata.n_vars}")
    print(f"数据稀疏度: {1 - np.count_nonzero(adata.X) / adata.X.size:.2%}")
    print(f"每个细胞平均检测到的基因数: {np.mean(np.sum(adata.X > 0, axis=1)):.0f}")
    print(f"每个基因在多少细胞中表达: {np.mean(np.sum(adata.X > 0, axis=0)):.0f}")
```

#### 主要挑战
1. **高维稀疏性**: 大部分基因在大部分细胞中不表达
2. **技术噪声**: 测序深度、扩增偏差等技术因素
3. **批次效应**: 不同实验批次间的系统性差异
4. **细胞异质性**: 细胞类型和状态的多样性

### 3.1.2 质量控制与过滤

#### 细胞质量控制
```python
def cell_quality_control(adata, min_genes=200, max_genes=5000, 
                        max_mito_pct=20, max_ribo_pct=50):
    """细胞质量控制和过滤"""
    # 计算质量指标
    adata.var['mt'] = adata.var_names.str.startswith('MT-')
    adata.var['ribo'] = adata.var_names.str.startswith(('RPS', 'RPL'))
    
    # 计算每个细胞的指标
    adata.obs['n_genes'] = np.sum(adata.X > 0, axis=1)
    adata.obs['total_counts'] = np.sum(adata.X, axis=1)
    adata.obs['mito_pct'] = np.sum(adata[:, adata.var['mt']].X, axis=1) / adata.obs['total_counts'] * 100
    adata.obs['ribo_pct'] = np.sum(adata[:, adata.var['ribo']].X, axis=1) / adata.obs['total_counts'] * 100
    
    # 过滤低质量细胞
    cell_filter = (
        (adata.obs['n_genes'] >= min_genes) &
        (adata.obs['n_genes'] <= max_genes) &
        (adata.obs['mito_pct'] <= max_mito_pct) &
        (adata.obs['ribo_pct'] <= max_ribo_pct)
    )
    
    print(f"过滤前细胞数: {adata.n_obs}")
    adata = adata[cell_filter, :].copy()
    print(f"过滤后细胞数: {adata.n_obs}")
    
    return adata
```

#### 基因过滤
```python
def gene_filtering(adata, min_cells=10, highly_variable=True, n_top_genes=2000):
    """基因过滤和高变基因选择"""
    # 过滤在少数细胞中表达的基因
    gene_filter = np.sum(adata.X > 0, axis=0) >= min_cells
    adata = adata[:, gene_filter].copy()
    
    if highly_variable:
        # 选择高变基因
        import scanpy as sc
        sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes)
        adata = adata[:, adata.var['highly_variable']].copy()
    
    return adata
```

### 3.1.3 跨物种数据处理

#### 同源基因映射
```python
def load_homologous_genes(homolog_file):
    """加载同源基因映射关系"""
    homolog_df = pd.read_csv(homolog_file, sep='\t')
    # 假设文件格式: human_gene, mouse_gene
    homolog_dict = dict(zip(homolog_df['mouse_gene'], homolog_df['human_gene']))
    return homolog_dict

def merge_species_data(species_u_adata, species_v_adata, homolog_dict=None):
    """合并不同物种的数据"""
    if homolog_dict is None:
        # 使用基因名直接匹配
        common_genes = list(set(species_u_adata.var_names) & set(species_v_adata.var_names))
    else:
        # 使用同源基因映射
        u_genes = set(species_u_adata.var_names)
        v_genes = set(species_v_adata.var_names)
        
        common_genes = []
        for u_gene in u_genes:
            if u_gene in homolog_dict and homolog_dict[u_gene] in v_genes:
                common_genes.append(u_gene)
    
    # 提取公共基因
    species_u_common = species_u_adata[:, common_genes].copy()
    species_v_common = species_v_adata[:, common_genes].copy()
    
    # 添加物种标识
    species_u_common.obs['species'] = 'species_u'
    species_v_common.obs['species'] = 'species_v'
    
    # 合并数据
    merged_adata = anndata.concat([species_u_common, species_v_common])
    
    return merged_adata
```

## 3.2 ICNN网络架构设计

### 3.2.1 理论基础回顾

#### 凸函数的性质
一个函数 $f: \mathbb{R}^n \rightarrow \mathbb{R}$ 是凸函数当且仅当：
$$f(\lambda x + (1-\lambda)y) \leq \lambda f(x) + (1-\lambda)f(y)$$
对所有 $x, y \in \mathbb{R}^n$ 和 $\lambda \in [0,1]$ 成立。

#### ICNN的构造原理
ICNN通过以下结构保证凸性：
$$z_{l+1} = \sigma(W_l z_l + A_l x + b_l)$$
其中：
- $W_l \geq 0$ (非负权重矩阵)
- $\sigma$ 是单调非递减的激活函数
- $A_l$ 可以是任意权重矩阵

### 3.2.2 网络实现细节

#### 非负线性层
```python
import torch
import torch.nn as nn

class NonNegativeLinear(nn.Linear):
    """确保权重非负的线性层"""
    def __init__(self, in_features, out_features, bias=True, beta=1.0):
        super().__init__(in_features, out_features, bias)
        self.beta = beta
    
    def forward(self, input):
        # 使用softplus确保权重非负
        positive_weight = nn.functional.softplus(self.weight, beta=self.beta)
        return nn.functional.linear(input, positive_weight, self.bias)
```

#### 完整ICNN实现
```python
class ICNN(nn.Module):
    """Input Convex Neural Network实现"""
    def __init__(self, input_dim, hidden_units, activation="LeakyReLU", 
                 softplus_beta=1.0, dropout_rate=0.1):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_units = hidden_units
        self.softplus_beta = softplus_beta
        
        # 激活函数
        if activation == "LeakyReLU":
            self.activation = nn.LeakyReLU(0.2)
        elif activation == "ReLU":
            self.activation = nn.ReLU()
        else:
            raise ValueError(f"不支持的激活函数: {activation}")
        
        # 构建网络层
        units = hidden_units + [1]  # 最后输出一个标量
        
        # W矩阵 (层间连接，必须非负)
        self.W_layers = nn.ModuleList([
            NonNegativeLinear(units[i], units[i+1], bias=False, beta=softplus_beta)
            for i in range(len(units)-1)
        ])
        
        # A矩阵 (输入连接，可以任意)
        self.A_layers = nn.ModuleList([
            nn.Linear(input_dim, units[i], bias=True)
            for i in range(len(units))
        ])
        
        # Dropout层
        self.dropout = nn.Dropout(dropout_rate)
        
        # 初始化权重
        self._initialize_weights()
    
    def _initialize_weights(self):
        """初始化网络权重"""
        for layer in self.A_layers:
            nn.init.xavier_uniform_(layer.weight)
            nn.init.zeros_(layer.bias)
        
        for layer in self.W_layers:
            nn.init.xavier_uniform_(layer.weight)
    
    def forward(self, x):
        """前向传播"""
        # 第一层：z_1 = σ(A_0 * x + b_0)
        z = self.activation(self.A_layers[0](x))
        z = self.dropout(z)
        
        # 中间层：z_{l+1} = σ(W_l * z_l + A_l * x + b_l)
        for i, (W_layer, A_layer) in enumerate(zip(self.W_layers[:-1], self.A_layers[1:-1])):
            z = self.activation(W_layer(z) + A_layer(x))
            z = self.dropout(z)
        
        # 输出层：y = W_{L-1} * z_{L-1} + A_{L-1} * x + b_{L-1}
        output = self.W_layers[-1](z) + self.A_layers[-1](x)
        
        return output
    
    def transport(self, x):
        """计算传输映射 T(x) = ∇f(x)"""
        if not x.requires_grad:
            x = x.requires_grad_(True)
        
        # 前向传播
        output = self.forward(x)
        
        # 计算梯度
        grad_outputs = torch.ones_like(output)
        gradients = torch.autograd.grad(
            outputs=output,
            inputs=x,
            grad_outputs=grad_outputs,
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]
        
        return gradients
```

### 3.2.3 网络训练技巧

#### 权重约束
```python
def clamp_weights(model):
    """手动约束权重为非负（如果不使用softplus）"""
    for name, param in model.named_parameters():
        if 'W_layers' in name and 'weight' in name:
            param.data.clamp_(min=0)
```

#### 梯度裁剪
```python
def clip_gradients(model, max_norm=1.0):
    """梯度裁剪防止梯度爆炸"""
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
```

---

**下一节预告**: 我们将继续学习多模态损失函数的设计和训练策略的实现。 